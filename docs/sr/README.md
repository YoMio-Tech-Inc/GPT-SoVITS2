# GPT-SoVITS2

Овај назив је одобрио аутор GPT-SoVITS-а, [花儿不哭](https://space.bilibili.com/5760446?spm_id_from=333.337.0.0).

### Овај пројекат је још увек у развоју и представља побољшање засновано на [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS). Главна побољшања су следећа:

1. **Нативна подршка за више језика**: Није ограничено на кинески, јапански и енглески, већ подржава било који језик на свету.
2. **Нема потребе за спецификацијом језика**: Увек је вишејезичан и можете слободно мешати језике при говору.
3. **Вишејезична екстракција емоција из текста**: Прецизнија анализа емоција у језицима, чинећи говор изражајнијим.
4. **Побољшање Zero Shot-а**: Сада се, уместо препоруке за фино подешавање модела, Zero Shot изводи директно користећи само неколико секунди циљног аудио записа.
5. **Фузија референтног аудио записа**: Могуће је учитати више референтних аудио снимака, а резултујући глас ће бити фузија више аудио снимака.
6. **Брже закључивање**: Промена позиционог уграђивања на RoPE, елиминишући потребу за поновним израчунавањем уграђивања целе секвенце за свако закључивање токена.

### **Позив за податке и сарадњу**: Тренутно се прикупљају подаци. QQ 1715069210, ако сет података испуњава захтеве, заслуге ће бити наведене у пројекту.

#### Тренутно се организују идеје за модификацију у изворном коду. Претражите # ! да бисте пронашли коментаре. Ако сте заинтересовани, слободно се обратите преко горе наведеног QQ-а.

### Листа промена

#### Промене у кодној књизи
Једна кодна књига -> 2 кодне књиге/4 кодне књиге

#### GPT промене
Замењено са qwen2-0.3b

#### Промене у аудио кодирању
cnhubert -> cnhubert-large/mHubert-147
Открио сам да је обука w2v-bert-2.0 мало тежа, обука mHubert-147 је релативно лакша, разлика у величини је четворострука, а у стварним тестовима, fp16 директно пада, тако да се може користити само fp32. Такође, mHubert је већ довољно велик (600MB).

#### Промене у текстуалном кодирању
Уклоњени фонеми и одговарајућа уграђивања
cn-roberta -> BGE-m3

#### Промене у позиционом кодирању
Текстуално и говорно кодирање свако ради синусоидално -> глобално се ради RoPE уграђивање.

#### Промене xy комбинованог уграђивања (експериментално)
Од оригиналног
x1+x2+y1 -> y2
промењено у
x1+y1+x2 -> y2
и цела секвенца дели RoPE уграђивање.
Теоретски, ово је погодније за проширивање више циљних аудио записа за фузију гласовних линија.
На пример:
x1+y1+x2+y2+x3+y3+x4+y4+x5 -> y5
делује природније него
x1+x2+x3+x4+x5+y1+y2+y3+y4 -> y5
Не може се строго доказати.

#### Димензионалне промене
MLP(768, 512) -> MLP(1024, 768)

#### Промене у методи обуке
Чисто ауторегресивно -> ауторегресивно + Zero Shot обука узорака регресије под истим говорником

#### vits промене
Могуће је наћи начин за повећање димензија. (256 -> 512) VITS -> VITS2 (углавном додавање transformer блока у модел протока).

#### Формат
Уједначена једнострука прецизност, hubert 16000 узорковање, vits 32000 узорковање, нормализација гласноће за све аудио записе.

#### Резиме
Укупно гледано, промене су углавном:
1. Коришћење напреднијих предобучених модела.
2. Пошто су напреднији модели већи, оригиналне димензије су такође проширене.
3. Због фокуса на Zero Shot, метод обуке укључује Zero Shot обуку.
4. Оригинални код је имао само bert за кинески, промена на BGE m3, вишејезично уграђивање, природно екстрахује све језике беспрекорно.
5. Оригинално је постојала само једна кодна књига величине 1024, чинећи оригинално hubert извлачење карактеристика недовољним. Промена на двоструке кодне књиге повећава количину информација на 1024^2 = 1048576, а четири кодне књиге су још екстремније, али би требало покушавати корак по корак због ограничених података.
6. Један од разлога за оригиналну спору брзину је то што GPT мора поново израчунавати уграђивање целе секвенце и позиционо уграђивање сваки пут. Прелазак на RoPE елиминише овај недостатак.
7. Првобитно, фузија гласовних линија није била разматрана, али касније сам модификовао грану у оригиналном GPT-SoVITS-у да постигнем фузију гласовних линија, иако то није био почетни циљ дизајна. У видеу 花-а, поменуто је да ли референтни аудио коришћен у GPT делу и референтни аудио у vits-у могу бити различити за фузију гласовних линија. Међутим, моја имплементација има више аудио записа у оба дела.
8. Промењени делови за које сам мислио да су неразумни. На пример, пошто се hubert користи као аудио уграђивање и кодне књиге као токени, зашто додати ar_audio_embedding. Такође, пошто је било фонема, морало им се доделити уграђивање, што је резултирало многим засебним обукама уграђивања, иако су bert и hubert већ коришћени. Штавише, одвојена текстуална и аудио уграђивања не би узимала у обзир контекст аудио и текста, боље је директно унети у GPT и користити пажњу за проналажење веза.

Ако сте прочитали до овде, то значи да разумете, и добродошли сте да се придружите овом пројекту!

**QQ: 1715069210**

**WeChat: JunityZ**

#### Брза напомена
Данас сам прочитао многе радове, укључујући VALLE2 рад, и имао многе нове идеје. Једно важно питање је да су тренутни ar_audio_embedding и ar_text_embedding историјски наслеђени проблеми.

Јер је audioLM прво користио hubert+kmeans за добијање токена, али пошто kmeans квантизација учења не треба да учи целокупне податке већ директно учи из hubert дистрибуције, додато је накнадно уграђивање.

Али ако се користи vq, vq сам по себи већ учи, тако да нема потребе за додавањем другог уграђивања. Овде су историјска питања увек додавала уграђивање, иако утицај можда није значајан, уклањање би било много разумније.

Штавише, audio lm је користио и семантичке и акустичне кроз hubert и soundstream, респективно. Али GPT SoVITS такође има ово, са meltransferencoder-ом који добија акустичне и hubert-ом који добија семантичке, што је веома случајно.

VALLE серије генерално користе EnCodec, који директно добија токене из аудио записа и треба друго уграђивање јер првобитно не даје уграђивање. Али коришћење hubert-а не треба ово јер је hubert-ов излаз већ уграђивање.

Обрнуто, ми добијамо токене са hubert уграђивањем, док EnCodec прво добија токене а затим врши уграђивање.

Стога се чини да су оригинални GPTSoVITS и претходно референцирани AUdio LM засновани на EnCodec серији TTS метода, али су заправо различити.

#### TODO
Поново написати квантизацију, директно позвати Group Residual VQ из vector-quantize-pytorch.
