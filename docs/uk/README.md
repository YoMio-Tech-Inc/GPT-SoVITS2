# GPT-SoVITS2

Ця назва була схвалена автором GPT-SoVITS, [花儿不哭](https://space.bilibili.com/5760446?spm_id_from=333.337.0.0).
### Цей проект все ще знаходиться в розробці і є вдосконаленням на основі [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS). Основні покращення полягають у наступному:

1. **Нативна підтримка кількох мов**: Не обмежується китайською, японською та англійською, а підтримує будь-яку мову світу.
2. **Немає потреби вказувати мову**: Завжди багатомовний, і ви можете вільно змішувати мови під час розмови.
3. **Багатомовне вилучення емоцій з тексту**: Більш точний емоційний аналіз мов, що робить мовлення більш виразним.
4. **Покращення Zero Shot**: Тепер замість рекомендації дотренування моделі, zero shot виконується безпосередньо з використанням лише кількох секунд цільового аудіо.
5. **Злиття референсного аудіо**: Можна завантажити кілька референсних аудіокліпів, і отриманий голос буде злиттям кількох аудіокліпів.
6. **Швидше виведення**: Зміна позиційного вбудовування на RoPE, що усуває необхідність перерахунку вбудовування всієї послідовності для кожного виведення токена.

### **Запит на дані та співпрацю**: Наразі відбувається збір даних. QQ 1715069210, якщо набір даних відповідає вимогам, кредит буде вказано у проекті.

#### На даний момент організовуються ідеї для модифікації у вихідному коді. Шукайте # ! для знаходження коментарів. Якщо зацікавлені, не соромтеся звертатися через вказаний вище QQ.

### Список змін

#### Зміни кодової книги
Одна кодова книга -> 2 кодові книги/4 кодові книги
#### Зміни GPT
Замінено на qwen2-0.3b
#### Зміни кодування аудіо
cnhubert -> ~~w2v-bert-2.0 (попередньо, це найбільш екстравагантне 4.6м-годинне багатомовне попереднє навчання, виконане meta. Якщо результат звучатиме як іноземець, що говорить китайською, буде замінено на cnhubert-large)~~/cnhubert-large/mHubert-147
Я виявив, що навчання w2v-bert-2.0 дещо складне, навчання mHubert-147 відносно легше, різниця в розмірі чотирикратна, і в реальних тестах fp16 прямо вилітає, тому можна використовувати лише fp32. Крім того, mHubert вже достатньо великий (600МБ).
#### Зміни кодування тексту
Видалення фонем та відповідних вбудовувань
cn-roberta -> BGE-m3
#### Зміни позиційного кодування
Кодування тексту та мовлення кожне робить синусоїдальне -> глобально робить вбудовування RoPE.
#### Зміни комбінованого вбудовування xy (експериментальні)
З оригінального
x1+x2+y1 -> y2
змінено на
x1+y1+x2 -> y2
і вся послідовність має спільне вбудовування RoPE.
Теоретично, це більше підходить для розширення більшої кількості цільового аудіо для злиття голосових ліній.
Наприклад:
x1+y1+x2+y2+x3+y3+x4+y4+x5 -> y5
відчувається більш природно, ніж
x1+x2+x3+x4+x5+y1+y2+y3+y4 -> y5
Не можна строго довести.
#### Зміни розмірностей
MLP(768, 512) -> ~~Без MLP, безпосередньо 1024 розмірності. Оскільки і w2v-bert-2.0, і bge-m3 мають 1024 розмірності, це ідеальне поєднання.~~ MLP(1024, 768)
#### Зміни методу навчання
Чисте авторегресійне -> авторегресійне + регресія зразка навчання zero shot під тим самим спікером
#### Зміни vits
Можливо, знайти спосіб збільшити розмірності. (256 -> 512) VITS -> VITS2 (в основному додавання блоку трансформера до моделі потоку).
#### Формат
Уніфікована ~~половинна точність~~ одинарна точність (реальні тести показали, що половинна точність вилітає), hubert 16000 семплювання, vits 32000 семплювання, нормалізація гучності для всього аудіо.
#### Підсумок
В цілому, зміни в основному такі:
1. Використання більш передових попередньо навчених моделей.
2. Оскільки більш передові моделі більші, оригінальні розмірності також розширені.
3. Через акцент на zero shot, метод навчання включає навчання zero shot.
4. Оригінальний код мав лише bert для китайської мови, змінено на BGE m3, багатомовне вбудовування, що природно витягує всі мови безперешкодно.
5. Оригінал мав лише одну кодову книгу розміром всього 1024, що робило оригінальне вилучення ознак hubert недостатнім. Зміна на подвійні кодові книги збільшує кількість інформації до 1024^2 = 1048576, а чотири кодові книги ще більш екстравагантні, але слід пробувати поступово через обмежені дані.
6. Одна з причин початкової повільної швидкості в тому, що GPT повинен перераховувати вбудовування всієї послідовності та позиційне вбудовування кожного разу. Перехід на RoPE усуває цей недолік.
7. Спочатку злиття голосових ліній не розглядалося, але пізніше я модифікував гілку в оригінальному GPT-SoVITS для досягнення злиття голосових ліній, хоча це не було початковою метою дизайну. У відео 花 було згадано, чи можуть референсне аудіо, використане в частині GPT, і референсне аудіо у vits бути різними для злиття голосових ліній. Однак у моїй реалізації є кілька аудіо в обох частинах.
8. Змінено частини, які я вважав нелогічними. Наприклад, оскільки hubert використовується як вбудовування аудіо, а кодові книги як токени, навіщо додавати ar_audio_embedding. Також, оскільки були фонеми, їм потрібно було призначити вбудовування, що призвело до навчання багатьох окремих вбудовувань, хоча bert і hubert вже використовувалися. Крім того, окремі вбудовування тексту та аудіо не враховували б контекст аудіо та тексту, краще безпосередньо ввести в GPT і використовувати увагу для пошуку зв'язків.

Якщо ви дочитали до цього місця, це означає, що ви розумієте, і ласкаво просимо приєднатися до цього проекту!

**QQ: 1715069210**

**WeChat: JunityZ**

#### Швидка замітка
Сьогодні я прочитав багато статей, включаючи статтю VALLE2, і у мене з'явилося багато нових ідей. Одне важливе питання полягає в тому, що поточні ar_audio_embedding та ar_text_embedding є історичними проблемами спадщини.

Тому що audioLM спочатку використовував hubert+kmeans для отримання токенів, але оскільки навчання квантизації kmeans не потребує вивчення загальних даних, а безпосередньо вчиться з розподілу hubert, додається подальше вбудовування.

Але якщо використовується vq, сам vq вже вчиться, тому немає потреби додавати ще одне вбудовування. Тут історичні проблеми завжди додавали вбудовування, хоча вплив може бути не значним, видалення було б набагато розумнішим.

Крім того, audio lm використовував як семантичний, так і акустичний аспекти через hubert та soundstream відповідно. Але GPT SoVITS також має це, з meltransferencoder, що отримує акустичний, і hubert, що отримує семантичний, що дуже збігається.

Серія VALLE зазвичай використовує EnCodec, який безпосередньо отримує токени з аудіо і потребує іншого вбудовування, оскільки спочатку не виводить вбудовування. Але використання hubert не потребує цього, оскільки вихід hubert вже є вбудовуванням.

І навпаки, ми отримуємо токени з вбудовуванням hubert, тоді як EnCodec спочатку отримує токени, а потім виконує вбудовування.

Тому оригінальний GPTSoVITS і попередній посилання на AUdio LM, здається, базуються на методах TTS серії EnCodec, але насправді вони відрізняються.

#### TODO
Переписати квантизацію, безпосередньо викликати Group Residual VQ з vector-quantize-pytorch.
