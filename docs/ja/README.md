# GPT-SoVITS2

この名前はGPT-SoVITSの作者[花儿不哭](https://space.bilibili.com/5760446?spm_id_from=333.337.0.0)の許可を得ています。
### このプロジェクトはまだ開発中であり、[GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS)に基づいて改良されました。主な改善点は以下の通りです：
|GPT-SoVITS|GPT-SoVITS2|
|:----:|:----:|
|**テキスト**|**テキスト**|
|テキスト->電話|テキスト->BPE|
|電話->埋め込み|BPE->埋め込み|
|ロベルタ-中国語|BGE-M3|
|**音声エンコーダー**|**音声エンコーダー**|
|ヒューバート|$S^3$|
|VQ|$S^3$->埋め込み|
|1024 音声トークン|4096 音声トークン|
|**AR**|**AR**|
|旧スタイルGPT|Qwen2-0.3b|
|**音声デコーダー**|**音声デコーダー**|
|VITS|VITS2|
|隠れサイズ192|隠れサイズ256|
|2 ヘッド|4 ヘッド|
|中間サイズ768|中間サイズ1024|
|**トレーニング**|**トレーニング**|
|ゼロショットトレーニングなし|同じ話者で異なる音声の推論|
|ZH,EN,JA|多言語|
|2000時間|まだ不明|

1. **多言語のネイティブサポート**：中国語、日本語、英語に限らず、世界中のどんな言語にも対応。
2. **言語を指定する必要がない**：いつでも多言語対応、自由に多言語を混ぜて話せます。
3. **多言語テキスト感情抽出**：言語の感情分析がより正確になり、話し方に感情が豊かになります。
4. **Zero Shotの向上**：今ではモデルの微調整を推奨せず、数秒のターゲット音声だけで直接ゼロショットします。
5. **参照音声の融合**：複数の参照音声をアップロードでき、得られる音声は複数の音声の融合結果になります。
6. **より高速な推論**：positional embeddingをRoPEに変更し、次のトークンを推論するたびにシーケンス全体のembeddingを再計算する必要がなくなります。

#### 現在、ソースコードの変更方針を整理しています。# ! を検索すると注釈が見つかります。興味がある方は上記のQQで交流を希望します。

### 変更リスト

#### コードブックの変更
~~シングルコードブック-> 2コードブック/4コードブック~~

$S^3$ の語彙サイズは4096の単一コードです。
#### GPTの変更
qwen2-0.3bに変更
#### 音声エンコードの変更
cnhubert-> ~~w2v-bert-2.0(暫定、これはmetaによる現在のトレーニングセットで最も壮大な4.6m時間多言語事前トレーニング。結果が外国人のように聞こえたらcnhubert-largeに変更)~~ ~~s/cnhubert-large/mHubert-147~~
~~sw2v-bert-2.0のトレーニングが少し難しいことが分かり、mHubert-147のトレーニングが比較的簡単で、サイズが四倍小さく、fp16では直接クラッシュするため、fp32のみ使用可能。また、mHubertは既に十分に大きい（600MB)。~~

CosyVoiceの$S^3$ エンコーダーを使用し、外部埋め込み層を接続。
#### テキストエンコードの変更
音素および対応するembeddingを削除

音素 -> BPEトークン化。

cn-roberta ->  BGE-m3
#### 位置エンコードの変更
テキストと音声エンコードをそれぞれsinusoidal->全体をRoPE embeddingに変更。
#### xy結合embeddingの変更（実験的）
元の
x1+x2+y1->y2
を
x1+y1+x2->y2
に変更し、シーケンス全体で1つのRoPE embeddingを共有
理論的には、これにより複数のターゲット音声をより自然に融合させることが可能
例えば
x1+y1+x2+y2+x3+y3+x4+y4+x5->y5
の方が
x1+x2+x3+x4+x5+y1+y2+y3+y4->y5
より自然に感じられる。厳密に証明することはできないが。
#### 次元の変更
MLP(768, 512) -> ~~MLPなしで直接1024次元。w2v-bert-2.0とbge-m3が共に1024次元であるため、完璧な組み合わせ~~ MLP(1024, 768)
#### トレーニング方法の変更
純粋な自己回帰->自己回帰+同一スピーカーのゼロショットトレーニングサンプルの回帰
#### vitsの変更
次元を拡大する方法を考える。(256->512) VITS -> VITS2 (主にフローモデルにtransformer blockを追加)
#### フォーマット
統一~~半精度~~単精度（テスト結果、半精度ではクラッシュ）、hubert 16000サンプル vits 32000サンプル、全ての音声の音量を統一
#### まとめ
全体的に見て、変更点は基本的に
1. より先進的な事前トレーニングモデルを使用
2. より先進的なモデルが大きくなるため、元の次元も拡大
3. ゼロショットを重視しているため、トレーニング方法にゼロショットトレーニングを追加
4. 元のコードは中国語だけにbertを使っていたが、BGE m3のような多言語embeddingに変更することで、全言語のシームレスな抽出が自然に可能
5. 元々シングルコードブックで、サイズが1024しかなかったため、hubert特徴抽出のガイド能力が不十分。ダブルコードブックに変更することで情報量が1024^2 = 1048576になり、4コードブックではさらに壮大だが、データがそれほど多くないため、一歩一歩試していく
6. 元々の遅さの原因の一つは、GPTが毎回シーケンス全体のembeddingとpositional embeddingを再計算する必要があったこと。しかしRoPEに変更することでこの欠点がなくなる
7. 元々声の融合に関心がなかったが、後に別のブランチを作成し、元のGPT-SoVITSに声の融合を実現した。しかし、最初の設計にはこの目標が全く含まれていなかった。花さんのビデオでは、GPT部分で使用される参照音声からの音声特徴とvitsでの参照音声が異なることが声の融合を可能にすると述べていた。しかし、私の実装では、両方の部分に複数の音声が含まれている
8. 私が合理的でないと感じた部分を変更した。例えば、既にhubertを音声のembeddingとして使用しているのに、なぜ再びar_audio_embeddingが必要なのか。そして、元々音素があったため、音素に対応するembeddingが必要で、多くの個別のembeddingが訓練されていたが、既にbertとhubertを使用していた。そして、個別のテキストembeddingと音声embeddingは文脈の音声とテキストを考慮していないため、直接GPTに入力しattentionで関係を見つける方が良い

ここまで読んでいただけたなら、理解していただけたと思います。このプロジェクトに参加することを歓迎します！

#### クイックノート
今日は多くの論文を読んでおり、VALLE2の論文からも新たなアイデアを得ました。重要な点は、現在のar_audio_embeddingとar_text_embeddingは歴史的な遺物であるということです。

audioLMが最初にhubert+kmeansを使用してトークンを取得しましたが、kmeanの量子化学習は全体のデータを学習する必要がなく、直接hubertの分布から学習するため、後続のembeddingが追加されました。

しかし、vqを使用すると、vq自体が学習を行っているため、vqに追加のembeddingは不要です。ここでの歴史的な問題はembeddingが常に追加されていることで、影響は大きくないが、削除するとより合理的です。

また、audio lmはsemanticとacousticの両方を使用しており、hubertとsoundstreamを通じてそれぞれ取得します。しかし、GPT SoVITSもこれを持っており、meltransferencoderはacousticを、hubertはsemanticを取得しますので非常に巧妙です。

VALLE系は一般的にEnCodecを使用しており、EnCodecは音声から直接トークンを取得するため、embeddingが再度必要です。hubertはその出力がembeddingであるため、不要です。

反対に、hubert embeddingを使用してトークンを取得し、EnCodecはトークンを取得した

後、embeddingを行います。

したがって、元のGPTSoVITSと以前の参考にしたAUdio LMはEnCodecベースのTTSに基づいた手法を参考にしているようですが、実際にはこれらは異なります。

#### TODO
量子化を再構築し、vector-quantize-pytorchのGroup Residual VQを直接使用
