# GPT-SoVITS2

Это название было одобрено автором GPT-SoVITS, [花儿不哭](https://space.bilibili.com/5760446?spm_id_from=333.337.0.0).

### Этот проект все еще находится в разработке и является улучшением на основе [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS). Основные улучшения заключаются в следующем:

1. **Нативная поддержка нескольких языков**: Не ограничивается китайским, японским и английским, а поддерживает любой язык мира.
2. **Нет необходимости указывать язык**: Он всегда многоязычен, и вы можете свободно смешивать языки при разговоре.
3. **Многоязычное извлечение эмоций из текста**: Более точный эмоциональный анализ языков, делающий речь более выразительной.
4. **Улучшение Zero Shot**: Теперь вместо рекомендации дообучения модели, zero shot выполняется напрямую, используя только несколько секунд целевого аудио.
5. **Слияние референсного аудио**: Можно загрузить несколько референсных аудиоклипов, и результирующий голос будет слиянием нескольких аудиоклипов.
6. **Более быстрый вывод**: Изменение позиционного кодирования на RoPE, устраняющее необходимость пересчитывать вложение всей последовательности для вывода каждого токена.

### **Запрос данных и сотрудничества**: В настоящее время идет сбор данных. QQ 1715069210, если набор данных соответствует требованиям, в проекте будет указано авторство.

#### В настоящее время идет организация идей модификации в исходном коде. Ищите # ! для поиска комментариев. Если интересно, не стесняйтесь обращаться через вышеуказанный QQ.

### Список изменений

#### Изменения в кодовой книге
Одна кодовая книга -> 2 кодовые книги/4 кодовые книги

#### Изменения в GPT
Заменено на qwen2-0.3b

#### Изменения в кодировании аудио
cnhubert -> ~~w2v-bert-2.0 (предварительно, это самое преувеличенное многоязычное предобучение на 4.6 миллионах часов, выполненное meta. Если результат будет звучать как иностранец, говорящий по-китайски, он будет заменен на cnhubert-large)~~/cnhubert-large/mHubert-147
Я обнаружил, что обучение w2v-bert-2.0 немного сложно, обучение mHubert-147 относительно легче, разница в размере четырехкратная, и в реальных тестах fp16 напрямую вызывает сбой, поэтому можно использовать только fp32. Кроме того, mHubert уже достаточно большой (600 МБ).

#### Изменения в кодировании текста
Удалены фонемы и соответствующие вложения
cn-roberta -> BGE-m3

#### Изменения в позиционном кодировании
Текстовое и речевое кодирование каждое делает синусоидальное -> глобально делается вложение RoPE.

#### Изменения в комбинированном вложении xy (экспериментально)
С оригинального
x1+x2+y1 -> y2
изменено на
x1+y1+x2 -> y2
и вся последовательность использует общее вложение RoPE.
Теоретически, это более подходит для расширения большего количества целевого аудио для слияния голосовых линий.
Например:
x1+y1+x2+y2+x3+y3+x4+y4+x5 -> y5
кажется более естественным, чем
x1+x2+x3+x4+x5+y1+y2+y3+y4 -> y5
Нельзя строго доказать.

#### Изменения в размерности
MLP(768, 512) -> ~~Нет MLP напрямую 1024 размерности. Поскольку и w2v-bert-2.0, и bge-m3 имеют размерность 1024, это идеальное совпадение.~~ MLP(1024, 768)

#### Изменения в методе обучения
Чистый авторегрессионный -> авторегрессионный + регрессия обучающей выборки zero shot под одним и тем же диктором

#### Изменения в vits
Возможно, найдется способ увеличить размерности. (256 -> 512) VITS -> VITS2 (в основном добавление блока трансформера к модели потока).

#### Формат
Унифицированная ~~половинная точность~~ одинарная точность (реальные тесты показали, что половинная точность вызывает сбои), hubert 16000 сэмплирование, vits 32000 сэмплирование, нормализация громкости для всего аудио.

#### Итог
В целом, изменения в основном заключаются в:
1. Использовании более продвинутых предобученных моделей.
2. Поскольку более продвинутые модели больше, оригинальные размерности также расширены.
3. Из-за фокуса на zero shot, метод обучения включает обучение zero shot.
4. В оригинальном коде был только bert для китайского языка, изменение на BGE m3, многоязычное вложение, естественно извлекает все языки бесшовно.
5. В оригинале была только одна кодовая книга размером всего 1024, что делало оригинальное извлечение признаков hubert недостаточным. Изменение на двойные кодовые книги увеличивает объем информации до 1024^2 = 1048576, а четыре кодовые книги еще более преувеличены, но следует пробовать постепенно из-за ограниченных данных.
6. Одна из причин изначальной медленной скорости заключается в том, что GPT должен пересчитывать вложение всей последовательности и позиционное вложение каждый раз. Переход на RoPE устраняет этот недостаток.
7. Изначально слияние голосовых линий не рассматривалось, но позже я модифицировал ветку в оригинальном GPT-SoVITS для достижения слияния голосовых линий, хотя это не было первоначальной целью дизайна. В видео 花 упоминалось, могут ли референсное аудио, используемое в части GPT, и референсное аудио в vits быть разными для слияния голосовых линий. Однако в моей реализации есть несколько аудио в обеих частях.
8. Изменены части, которые я считал нецелесообразными. Например, поскольку hubert используется как аудиовложение, а кодовые книги как токены, зачем добавлять ar_audio_embedding. Также, поскольку были фонемы, им нужно было назначить вложение, что приводило к обучению многих отдельных вложений, хотя уже использовались bert и hubert. Более того, отдельные текстовые и аудиовложения не учитывали бы контекст аудио и текста, лучше напрямую вводить в GPT и использовать внимание для поиска связей.

Если вы дочитали до этого места, значит, вы понимаете, и вы приглашаетесь присоединиться к этому проекту!

**QQ: 1715069210**

**WeChat: JunityZ**

#### Быстрая заметка
Сегодня я прочитал много статей, включая статью VALLE2, и у меня появилось много новых идей. Одна важная проблема заключается в том, что текущие ar_audio_embedding и ar_text_embedding являются историческими проблемами наследия.

Потому что audioLM сначала использовал hubert+kmeans для получения токенов, но поскольку обучение квантования kmeans не нуждается в обучении на всех данных, а напрямую учится из распределения hubert, добавляется последующее вложение.

Но если используется vq, сам vq уже обучается, поэтому нет необходимости добавлять еще одно вложение. Здесь исторические проблемы всегда добавляли вложение, хотя влияние может быть не значительным, удаление его было бы гораздо более разумным.

Более того, audio lm использовал как семантическое, так и акустическое через hubert и soundstream соответственно. Но GPT SoVITS также имеет это, с meltransferencoder, получающим акустическое, и hubert, получающим семантическое, что очень совпадает.

Серия VALLE обычно использует EnCodec, который напрямую получает токены из аудио и нуждается в другом вложении, так как изначально не выводит вложение. Но использование hubert не нуждается в этом, потому что выход hubert уже является вложением.

Наоборот, мы получаем токены с помощью вложения hubert, в то время как EnCodec сначала получает токены, а затем выполняет вложение.

Поэтому оригинальный GPTSoVITS и ранее упомянутый AUdio LM, кажется, основаны на методах TTS серии EnCodec, но на самом деле они отличаются.

#### TODO
Переписать квантование, напрямую вызывать Group Residual VQ из vector-quantize-pytorch.
